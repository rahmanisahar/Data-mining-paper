%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%Method
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

\section{METHOD}
 \subsection{Self Organizing Maps}
 
 Kohonen Self organizing map (or self organizing map, SOM) is an unsupervised neural network for mapping and visualizing a complex and non linear high dimension data introduced by \citep{Kohonen82}. 
 SOM is a clustering method which reduces the dimension of data to lower dimensions usually 1 or 2D while preserving topological features of the original data.
 Results of SOM contains nodes (usually hexagonal ones) that arranged in 1D or 2D arrays.
 Each nodes may contain one or more samples from input data and distance between nodes represents similarity or dissimilarity of underlying samples. 
 In the way that similar data are closer together in the array and further nodes go, they become more different.
 %Sahar_self_notes: maybe "some" is better instead of one or more
 Also, a weight vector ``$w$" with same dimension of the input data associates with each node.
 Thus the SOM is a technique that shows a simple geometry relationship of a non-linear high dimension data on a map \citep{Kohonen98}. 
   \subsubsection{Algorithm of SOM} 
   
 Having a data which contains vectors, $V \in \Re^n$. Therefore, each node contains a weight vectors $w\in \Re^n$. The process of creating SOM, happens over series of $N$ iterations. For each iteration :
   
  1) assigning a random weight vector
  
  2) from the data a vector will be choose randomly and present to a network
  
  3) calculating the Euclidean distance for each node j as  $D_j^2= \sum_{i=0}^{i=n} (V_i - w_i)^2 $ to find the smallest value for "$D_{j_{min}}$". The winner node is calling Best Matching Unit (BMU)
  
  4) Compute the radius of the neighbourhood of the BMU to find nodes that all are within the neighbourhood distance of the BMU. The weight vectors of these nodes will be affected in the next steps. This value is arbitrary but initially can be chosen to be half of the size of the SOM and then it decades exponentially over the time:
  %Sahar_self_note: or Whatever matlab prefers
   \begin{equation}
   r(t) = r^0_{BMU}e^{(-t/\tau)}
   \end{equation}
   where $\tau$ is a decay constant and usually set to be the same as number of iterations, $N$.
   
   5) The weight vector of every nodes within the neighbourhood will be adjusted as:
   \begin{equation}
   w(t+1)=w(t)+L(t) \times(v(t)-w(t))
   \end{equation}
   where $L(t) = e^{(-t/\tau}$ is the learning factor which decrease with time.
   
   6) Repeat these steps for $N$ times.
   
   
   
 \subsection{Test Model}
 Before we start using SOM on our data, we apply this model in a test sample. Our test sample contains SED of 142 galaxies between redshift 0.5 $ \leq$ Z $\leq $ 1, which their physical information such as age, stellar mass and specific star formation are available. \cite{Hossein12} used trained network to classified their spectrum based on SED models from \cite{Kinney96} and \cite{Coleman80}. 
 
 We create various SOM maps with different numbers of neurons and neighbourhood 
