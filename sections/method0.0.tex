%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%Method
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

\section{METHOD}
\label{sec: method}
 \subsection{Self Organizing Maps}
 \label{sec: som}
 
 Kohonen Self organizing map (or self organizing map, SOM) is an unsupervised neural network for mapping and visualizing a complex and non linear high dimension data introduced by~\citep{Kohonen82}. 
 The SOM is a technique that shows a simple geometry relationship of a non-linear high dimension data on a map \citep{Kohonen98}.
 The SOM is a clustering method which reduces the dimension of data to lower dimensions usually 1 or 2D while preserving topological features of the original data.
 Results of SOM contains nodes (usually hexagonal ones) that arranged in 1D or 2D arrays.
 
  
 Each nodes may contain one or more samples from input data and distance between nodes represents similarity or dissimilarity of underlying samples. 
 In the way that similar data are closer together in the array and the further nodes go from each other, the more dissimilarity appears between their samples.
 %Sahar_self_notes: maybe "some" is better instead of one or more
 Moreover,a weight vector ``\boldit{W}" with same dimension of the input data associates with each node which will be change during the process and has a key factor in a position of nodes in the map. 
 
 \subsubsection{Algorithm of SOM} 
 \label{sec: algorithm}

     Assuming we have a data set which contains vectors, \boldit{V} $\in \Re^n$ and we want to map them on S1 by S2 map. 
     We start by creating S1 $\times$ S2 empty neurons. 
     The initial arrangement of these neurons depends on a map's topology provided by user.
     Since the topology of the map does not have any affect on the final result, we chose hexagonal topology which is the default topology for SOMs.
     Then, we assign a random weight vector \boldit{W} $\in \Re^n$ to each node.
     The process of creating SOM, happens over series of $N$ iterations. 
     During each iteration the weight vectors might change according to the Kohonen learning rule (equation~\ref{equ: weight adj}). 
      In each iteration:
     \begin{enumerate}
     \item Choose a random vector from our data set.
     \item Calculate the euclidean distance for each node j as  $D_j^2= \sum_{i=0}^{i=n} (V_i - W_i)^2$, and find a neuron with ``$D_{j_{min}}$". This neuron is the winner node and is calling Best Matching Unit (BMU). 
     \item  Compute the radius of the neighbourhood of the BMU to find nodes within this radius. The weight vectors of these nodes will be affected in the next steps. This value is arbitrary and initially can be set to be as high as half of the SOM size and then it decades exponentially over each iteration:
   \begin{equation}
   r^t_{BMU} = r^0_{BMU}e^{(-t/\tau)}
   \end{equation}
   where $\tau$ is a decay constant and usually set to be the same as number of iterations, $N$. $r^0_{BMU}$ and $r^t_{BMU}$ is the radius of the neighbourhood at 0th and $t$th iteration, respectively. 
   \item Change the weight vectors of the BMU and all the nodes within r(t) as:
   \begin{equation}
   \label{equ: weight adj}
   w(t+1)=w(t)+L(t) \times R(t) \times(v(t)-w(t))
   \end{equation}
   where $L(t) = e^{(-t/\tau)}$ is the learning factor which prevents divergence of the SOM and $R(t)=exp(-\frac{D_j^2}{2r^t_{BMU}})$ is the influence rate. $R(t)$ determines how weight of nodes in the neighbourhood of BMU will change.
   \item  Repeat these steps for $N$ times.
    \end{enumerate}
   
    In order to create SOM, we used {\tiny MATLAB} neural network toolbox~\citep{sommatlab}. 
    We created different SOM with different sizes, initial neighbourhood, and iteration numbers to find the optimize result for our sample.
    We test our code by varying the number of iterations and find $N = 100$ is optimized for our sample.
    In Section~\ref{sec: test model} we used this toolbox to create SOMs from SED of galaxies and learn how this method works and how we can interpret the results.
   
 \subsection{Test Model}
 \label{sec: test model}
    Before we start using SOM on our data, we apply this model in a test sample. 
    Our test sample contains SED of 142 galaxies between redshift 0.5 $ \leq$ Z $\leq $ 1, which their physical information such as age, stellar mass and specific star formation are available. 
    \cite{Hossein12} used trained network to classified their spectrum based on SED models from \cite{Kinney96} and \cite{Coleman80}.
    
    We create various SOM maps with different numbers of neurons and neighbourhood 
