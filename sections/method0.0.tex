%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%Method
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

\section{METHOD}
\label{sec: method}
 \subsection{Self Organizing Maps}
 \label{sec: som}
 The SOM is a clustering method which reduces the dimension of data to lower dimensions, while preserving topological features of the original data~\citep{Kohonen98}. 
 The results of the SOM are shown with a map of neurons.
 Each neuron has a fixed position in the map, and may contain one or more samples from input data, \boldit{V} $\in \Re^n$.
 A weight vector,~\boldit{W} $\in \Re^n$, with the same dimension as the input data is associated with each node and will be varied during the training process.
 The process of creating SOM, happens over series of $N$ iterations.
 In each iteration, the algorithm calculates the euclidean distance for each node j as  $D_j^2= \sum_{i=0}^{i=n} (V_i - W_i)^2$, and finds a neuron with ``$D_{j_{min}}$". 
 This neuron is the winner node and is calling Best Matching Unit (BMU). 
 The weight vectors in the neighbourhood of BMU will change according to the Kohonen learning rule (equation~\ref{equ: weight adj}). 
  \begin{equation}
            \label{equ: weight adj}
            w(t+1)=w(t)+L(t) \times R(t) \times(v(t)-w(t))
 \end{equation}
where $L(t) = L_0 e^{(-t/\tau)}$ is the learning factor, which prevents the divergence of the SOM and $R(t)=\exp(-\frac{D_j^2}{2r^t_{BMU}})$ is the influence rate, which determines how the weight of each node will change.
Values for number of neurons, L(t) and R(t) are arbitrary. 
~\cite{Geach12} and \cite{Rahmani16b} demonstrate the algorithm of the SOM in more details.


     In order to create SOM, we used {\sc MATLAB} neural network toolbox~\citep[NNT,][]{matlabtolbox}.
     An SOM in {\sc NNT} can be created by {\sc newsom} library which works in two phases; ``ordering phase" and ``tuning phase" . 
     Phase one is the ``ordering phase". 
     This phase starts with maximum neighbourhood distance, and initial high learning factor usually 0.9 which is provided by user. 
     The ordering phase continues for requested number of iterations and it continues till the learning factor reduces to tuning phase leaning factor and the neighbourhood distance reaches to the number set by the user.
     
     The second phase is the ``tuning phase".
     In this phase the neighbourhood distance is at its minimum, but learning factor decreases very slowly.
     This minimum neighbourhood distance and slowly decreasing the leaning factor helps to fine tune the topology results and causes the more stable SOM. 
     The number of iterations in this tuning phase most be much more than the number iterations in ordering phase, to allow the tuning happens slowly. 
     We chose number of epochs the tuning phase be 3 times more than number of epochs in the ordering phase.
     
     To present our results, we use {\sc nnt}'s built-in plotting tool.
     Specifically, we combine two of the plots in this tool: a hits map, which shows the number of times each neuron has became the winner (hits), and a distance map, which shows the distance between weight vectors of those neurons.
     In the maps, the purple hexagonal shapes shows the neurons. 
     The distances are shown by the grey cycle colours: the darker the colour, the larger the distance between neurons.
     Neurons with zero hits are left empty.
     In Section~\ref{sec: mock_sample} we used {\sc newsom} to create SOMs from a mock sample to illustrate how this method works and how we can interpret the results.

     One feature of the SOM method is that there is no rule and restriction on number of clusters.
     Users must decide the size of networks based on their data set and their usages of the results .
     There has been few attempts to find a rule for restricting the number of neurons based on the input sample \citep[e.g.][]{Vesanto05}, but none of them are certain. 
     We varied size of SOMs from $1\times2$ to $50\times50$ to choose the most suitable size for our maps, and found that based on the size of our sample, the 2D SOMs to be 10$\times$10. 
     We create the final SOMs with initial values for number of iterations in ordering phase, ordering phase learning factor, tuning phase learning factor, and tuning phase neighbourhood distance of 500, 0.9, 0.02, and 1, respectively. 
     %Also for each grid we created different SOM with different, learning factors, neighbourhood distances, and iteration numbers to find the optimize result for our sample.
     %Based on our data we created our final SOMs with following initial values: number of iteration in ordering phase = 1000; ordering phase learning factor = 0.9; tuning phase learning factor= 0.02; and tuning phase neighbourhood distance to be 1.
     %All the other parameters are default values in {\sc newsom} library and cannot be changed by the users. 
     
    
\subsection{Mock sample}
\label{sec: mock_sample}
 
         \begin{figure}
                \centering
                \includegraphics[width=0.5\textwidth]{../images0.01/mock_sample.png}
            \caption{SOM of the mock sample. The axes show the position of the neurons. Hexagonal shapes represent the neurons. The grey cycle colours show the differences between weight of each neuron where white is the minimum differences and black has the maximum one.}
            \label{fig: sample}
        \end{figure}
 
To show how self-organizing maps work, we created a mock sample, which contains a few regions.
We gave two information to each sample; amount of dust and the SFR.
 Therefore, we have a simple input data with low dimension; each entry only can get either 0 or 1 as high or low SFR, and 0 or 0.5 to show high or low amount of dust for the mock regions.
 We generated an SOM with the size of $10 \times 10$ from the sample, using the method that described in Sec.~\ref{sec: som}.
 Fig. ~\ref{fig: sample} shows the SOM of the mock sample. 
 The axes show the position of the neurons in a $10 \times 10$ network and the hexagonal shapes are the neurons.
 
Using this method, as expected, we are able to divide the mock sample into 4 distinct groups: regions with high SFR and high amount of dust, regions with low SFR and high amount of dust, regions with high SFR and low amount of dust, and regions low SFR and low amount of dust. 
The plot in the Fig.~\ref{fig: sample}, clearly shows these divisions.
In that plot, the upper part belongs to regions with low amount of dust, while the lower part belongs to the high amount of dust.
The left part of the plot, is where regions with low SFR belong to and the right side is for high SFR regions.
Grey to black colours show the border between regions.
This network is considered as a trained network, and can used to cluster any new data set with similar entries.

Having two regions with exactly the same values in all their quantities in the real world is almost impossible. 
Therefore, one can find a network with high amount of the neurons, which separates the input data completely, and clusters them into separate groups.
However, if the input data has high similarity, the number of neurons must be much higher than the number of input samples to be able to separate the groups from each other. 
Therefore, it is up to the user to decide the similarity or dissimilarity between the input data based on the number of neurons. 